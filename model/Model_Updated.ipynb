{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rajendrak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from transformers import CLIPTokenizer\n",
    "from diffusers import StableDiffusionPipeline, UNet2DModel, AutoencoderKL\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class LogoDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_folder, tokenizer, image_size=(256, 256)):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.image_folder = image_folder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),  # Normalize to [-1, 1]\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        prompt = row['Description']\n",
    "        image_path = f\"{self.image_folder}/{row['Name']}\"\n",
    "\n",
    "        tokens = self.tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=77, return_tensors=\"pt\")\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return tokens['input_ids'].squeeze(), tokens['attention_mask'].squeeze(), image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset and dataloader\n",
    "csv_path = \"data.csv\"\n",
    "image_folder = \"dataset\"\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LogoDataset(csv_path, image_folder, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)  # Smaller batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]c:\\Users\\rajendrak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rajendrak\\.cache\\huggingface\\hub\\models--stabilityai--stable-diffusion-2-1-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 13 files: 100%|██████████| 13/13 [10:18<00:00, 47.59s/it]\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00,  7.04it/s]\n"
     ]
    }
   ],
   "source": [
    "pipeline = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(pipeline.unet.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 5\n",
    "accumulation_steps = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   1%|          | 2/333 [02:50<8:05:39, 88.03s/it]"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for step, (input_ids, attention_mask, images) in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
    "        images = images.to(device)\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "\n",
    "        # Encode images to latent space using VAE\n",
    "        with torch.no_grad():\n",
    "            latents = pipeline.vae.encode(images).latent_dist.sample() * 0.18215\n",
    "\n",
    "        # Generate text embeddings using CLIP text encoder\n",
    "        with torch.no_grad():\n",
    "            text_embeddings = pipeline.text_encoder(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(0, 1000, (latents.size(0),), device=latents.device).long()\n",
    "\n",
    "        # Add noise to latents\n",
    "        noisy_latents = latents + noise * 0.1\n",
    "\n",
    "        # UNet forward pass with text conditioning\n",
    "        model_output = pipeline.unet(noisy_latents, timesteps, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "        # Compute MSE loss\n",
    "        loss = nn.functional.mse_loss(model_output, noise)\n",
    "        loss = loss / accumulation_steps  # Normalize for gradient accumulation\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        if (step + 1) % accumulation_steps == 0 or (step + 1) == len(dataloader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create directory to save models and outputs\n",
    "save_dir = \"trained_model\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save model components and tokenizer\n",
    "def save_model_and_tokenizer(pipeline, save_dir):\n",
    "    pipeline.unet.save_pretrained(os.path.join(save_dir, \"unet\"))\n",
    "    pipeline.vae.save_pretrained(os.path.join(save_dir, \"vae\"))\n",
    "    pipeline.text_encoder.save_pretrained(os.path.join(save_dir, \"text_encoder\"))\n",
    "    pipeline.tokenizer.save_pretrained(os.path.join(save_dir, \"tokenizer\"))\n",
    "    print(f\"Model components and tokenizer saved to {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Function for Performance Metrics\n",
    "def validate_model(pipeline, dataloader, device, num_samples=8):\n",
    "    pipeline.eval()\n",
    "    total_mse_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (input_ids, attention_mask, images) in enumerate(tqdm(dataloader, desc=\"Validating\")):\n",
    "            if step >= num_samples:  # Evaluate only on a few samples for performance\n",
    "                break\n",
    "            \n",
    "            images = images.to(device)\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "\n",
    "            # Encode images to latent space\n",
    "            latents = pipeline.vae.encode(images).latent_dist.sample() * 0.18215\n",
    "\n",
    "            # Generate text embeddings\n",
    "            text_embeddings = pipeline.text_encoder(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "            # Add noise to latents\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, 1000, (latents.size(0),), device=latents.device).long()\n",
    "            noisy_latents = latents + noise * 0.1\n",
    "\n",
    "            # UNet forward pass\n",
    "            predicted_noise = pipeline.unet(noisy_latents, timesteps, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "            # Compute MSE loss\n",
    "            mse_loss = nn.functional.mse_loss(predicted_noise, noise)\n",
    "            total_mse_loss += mse_loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_mse_loss = total_mse_loss / num_batches\n",
    "    print(f\"Validation MSE Loss: {avg_mse_loss:.4f}\")\n",
    "    return avg_mse_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Sample Images\n",
    "def generate_samples(pipeline, prompts, output_dir, num_steps=50):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    pipeline.eval()\n",
    "\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        with torch.no_grad():\n",
    "            # Tokenize prompt\n",
    "            inputs = pipeline.tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=77)\n",
    "            input_ids = inputs.input_ids.to(device)\n",
    "            attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "            # Generate text embeddings\n",
    "            text_embeddings = pipeline.text_encoder(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "            # Random noise\n",
    "            latents = torch.randn((1, pipeline.unet.in_channels, 64, 64), device=device)\n",
    "\n",
    "            # Denoise latents\n",
    "            for t in reversed(range(num_steps)):\n",
    "                timestep = torch.full((1,), t, device=device, dtype=torch.long)\n",
    "                noise_pred = pipeline.unet(latents, timestep, encoder_hidden_states=text_embeddings).sample\n",
    "                latents = latents - noise_pred * 0.1  # Step adjustment for denoising\n",
    "\n",
    "            # Decode latents to images\n",
    "            images = pipeline.vae.decode(latents / 0.18215).sample\n",
    "            save_image(images, os.path.join(output_dir, f\"sample_{i}.png\"))\n",
    "            print(f\"Generated image for prompt '{prompt}' saved as sample_{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "save_model_and_tokenizer(pipeline, save_dir)\n",
    "\n",
    "# Validate Model\n",
    "validation_loss = validate_model(pipeline, dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Samples\n",
    "sample_prompts = [\"A modern minimalist logo\", \"A vintage-style floral logo\", \"An abstract tech-themed logo\"]\n",
    "generate_samples(pipeline, sample_prompts, output_dir=\"generated_samples\")\n",
    "\n",
    "print(\"Model saved, validation completed, and samples generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
