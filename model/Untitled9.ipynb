{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5CteEMDSUmo",
        "outputId": "b2943e67-de7b-4620-de0b-da23023138ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: torch==2.4.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.4.1+cu121)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.1->torchvision) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.1->torchvision) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets torchvision Pillow\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the modern logo dataset\n",
        "dataset = load_dataset(\"logo-wizard/modern-logo-dataset\")\n",
        "\n",
        "# Display the structure of the dataset\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHCUeLM4bEX9",
        "outputId": "fb6166f0-2851-496d-9dbc-baa587fd5d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['image', 'text'],\n",
            "        num_rows: 803\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few samples\n",
        "for i in range(5):\n",
        "    print(dataset['train'][i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kw47uOwlbEab",
        "outputId": "ac7d23ac-a484-4c74-c232-86d550c03e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=904x938 at 0x7A96B895BD60>, 'text': '\"a logo of coffee shop, take-away coffee cardboard glass with white and brown stripes and dark brown lid, coffee circle with three cream drops, white background, brown foreground, minimalism, modern\"'}\n",
            "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=968x928 at 0x7A96B89CE560>, 'text': '\"a logo of coffee shop, White round background with black rim, cup, pretzel, horizontal stripe and cookery lettering, tan background, snow, darkslategray foreground, minimalism, modern\"'}\n",
            "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1060x1040 at 0x7A96B89CE650>, 'text': '\"a logo of coffee shop, image of a filled cup with steam in a square, white background, black, darkgray foreground, minimalism, modern\"'}\n",
            "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=588x580 at 0x7A96B895B7F0>, 'text': '\"a logo of cafe restaurant bar pizzeria with a slice of pizza at the top part of the circle, label with the year \\'2020\\' at the bottom and crossed rectangle in the middle, whitesmoke background, darkslategray, dimgray foreground, minimalism, modern\"'}\n",
            "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=1304x970 at 0x7A96B89CE650>, 'text': '\"a logo of cafe restaurant bar with a circle with an ornament on the sides, tableware above and two leaves on top and bottom, lightcyan background, midnightblue, midnightblue foreground, minimalism, modern\"'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the features of the dataset\n",
        "print(dataset['train'].features)\n",
        "\n",
        "# Display a sample item to see its structure\n",
        "print(dataset['train'][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEAPJcijbdCP",
        "outputId": "e64ecdf3-e737-499c-a7a1-f90b487f36c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'image': Image(mode=None, decode=True, id=None), 'text': Value(dtype='string', id=None)}\n",
            "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=904x938 at 0x7A96B89CE740>, 'text': '\"a logo of coffee shop, take-away coffee cardboard glass with white and brown stripes and dark brown lid, coffee circle with three cream drops, white background, brown foreground, minimalism, modern\"'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resize the image to 128x128\n",
        "    transforms.ToTensor(),            # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
        "])\n"
      ],
      "metadata": {
        "id": "0_bwXwqhkYWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import ToPILImage\n",
        "\n",
        "# Load the modern logo dataset\n",
        "dataset = load_dataset(\"logo-wizard/modern-logo-dataset\")\n",
        "\n",
        "# Define the image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resize the image to 128x128\n",
        "    transforms.ToTensor(),            # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "def preprocess_data(dataset):\n",
        "    images, descriptions = [], []\n",
        "\n",
        "    for item in dataset['train']:\n",
        "        image = item['image']  # This is already a PIL image\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')  # Convert to RGB if not already\n",
        "        image = transform(image)  # Apply transformations\n",
        "        images.append(image)\n",
        "        descriptions.append(item['text'])  # Use the correct key for descriptions\n",
        "\n",
        "    return images, descriptions\n",
        "\n",
        "# Call the preprocessing function\n",
        "images, descriptions = preprocess_data(dataset)\n",
        "\n",
        "# Instantiate the conversion transformation\n",
        "to_pil = ToPILImage()\n",
        "\n",
        "# Display a sample image and description\n",
        "sample_index = 0\n",
        "image_to_show = to_pil(images[sample_index])\n",
        "image_to_show.show()\n",
        "print(descriptions[sample_index])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgBaaJNlkaB6",
        "outputId": "cb703b7a-df20-4ce4-f51c-6f72ec85efd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"a logo of coffee shop, take-away coffee cardboard glass with white and brown stripes and dark brown lid, coffee circle with three cream drops, white background, brown foreground, minimalism, modern\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of samples\n",
        "print(f'Number of images: {len(images)}')\n",
        "print(f'Number of descriptions: {len(descriptions)}')\n",
        "\n",
        "# Display a sample image and description\n",
        "sample_index = 0\n",
        "image_to_show = to_pil(images[sample_index])  # Convert tensor to PIL image\n",
        "image_to_show.show()\n",
        "print(descriptions[sample_index])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rTsMm6VTF4M",
        "outputId": "8358c92e-4257-4bf6-bcf2-d9792fdbbf09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images: 803\n",
            "Number of descriptions: 803\n",
            "\"a logo of coffee shop, take-away coffee cardboard glass with white and brown stripes and dark brown lid, coffee circle with three cream drops, white background, brown foreground, minimalism, modern\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, text_embedding_size=100):  # Adjust this based on your actual embedding size\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(100 + text_embedding_size, 256),  # Update this size\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 49152),  # Update output size if needed\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, text_embedding):\n",
        "        combined_input = torch.cat((noise, text_embedding), dim=1)\n",
        "        return self.model(combined_input)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PQOo0X4JTGGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "J0UiuoIInP3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(generator, images, descriptions, epochs=10, batch_size=32):\n",
        "    criterion = nn.BCELoss()  # Define loss function\n",
        "    optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, len(images), batch_size):\n",
        "            # Get batch of images\n",
        "            batch_images = images[i:i + batch_size]\n",
        "            batch_images = torch.stack(batch_images).to(device)  # Move to GPU if available\n",
        "\n",
        "            # Create random noise\n",
        "            noise = torch.randn(batch_size, 100).to(device)  # Adjust noise size\n",
        "            generated_images = generator(noise)\n",
        "\n",
        "            # Calculate loss (you'll need to define how to compare generated with real)\n",
        "            # Loss calculation logic goes here\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{epochs} completed')\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "generator.to(device)\n",
        "\n",
        "# Start training (commented out for now; will implement loss calculation first)\n",
        "# train_model(generator, images, descriptions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSnpoqAjcgvB",
        "outputId": "29976384-3928-4740-bbb7-567260ea52e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Generator(\n",
              "  (model): Sequential(\n",
              "    (0): Linear(in_features=100, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=512, out_features=49152, bias=True)\n",
              "    (5): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(49152, 512),  # Adjust based on your image size\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1),  # Output a single value for real/fake\n",
        "            nn.Sigmoid()  # Use Sigmoid for binary classification\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "Wu9JT5iDnzz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = Discriminator().to(device)\n"
      ],
      "metadata": {
        "id": "DrIkemGsn1nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(generator, discriminator, images, descriptions, epochs=10, batch_size=32):\n",
        "    for epoch in range(epochs):\n",
        "        d_loss_epoch = 0\n",
        "        g_loss_epoch = 0\n",
        "        total_batches = len(images) // batch_size\n",
        "\n",
        "        for i in range(0, len(images), batch_size):\n",
        "            # Get a batch of images\n",
        "            batch_images = images[i:i + batch_size]\n",
        "            if len(batch_images) < batch_size:\n",
        "                continue  # Skip if the last batch is smaller\n",
        "\n",
        "            batch_images = torch.stack(batch_images).to(device)\n",
        "\n",
        "            # Create random noise for the generator\n",
        "            noise = torch.randn(batch_images.size(0), 100).to(device)  # Use current batch size\n",
        "            generated_images = generator(noise)\n",
        "\n",
        "            # Create labels for real and fake images\n",
        "            real_labels = torch.ones(batch_images.size(0), 1).to(device)\n",
        "            fake_labels = torch.zeros(batch_images.size(0), 1).to(device)\n",
        "\n",
        "            # Train Discriminator\n",
        "            optimizer_d.zero_grad()\n",
        "            outputs = discriminator(batch_images.view(batch_images.size(0), -1))\n",
        "            d_loss_real = criterion(outputs, real_labels)\n",
        "\n",
        "            outputs = discriminator(generated_images.detach().view(generated_images.size(0), -1))\n",
        "            d_loss_fake = criterion(outputs, fake_labels)\n",
        "\n",
        "            d_loss = d_loss_real + d_loss_fake\n",
        "            d_loss.backward()\n",
        "            optimizer_d.step()\n",
        "\n",
        "            # Train Generator\n",
        "            optimizer_g.zero_grad()\n",
        "            outputs = discriminator(generated_images.view(generated_images.size(0), -1))\n",
        "            g_loss = criterion(outputs, real_labels)\n",
        "\n",
        "            g_loss.backward()\n",
        "            optimizer_g.step()\n",
        "\n",
        "            # Accumulate losses for the epoch\n",
        "            d_loss_epoch += d_loss.item()\n",
        "            g_loss_epoch += g_loss.item()\n",
        "\n",
        "            # Log losses and progress\n",
        "            if i % (batch_size * 10) == 0:  # Print every 10 batches\n",
        "                print(f'Epoch [{epoch + 1}/{epochs}], Step [{i // batch_size + 1}/{total_batches}], '\n",
        "                      f'D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}')\n",
        "\n",
        "        # Print average losses for the epoch\n",
        "        print(f'End of Epoch [{epoch + 1}/{epochs}], Avg D Loss: {d_loss_epoch / total_batches:.4f}, Avg G Loss: {g_loss_epoch / total_batches:.4f}')\n",
        "\n",
        "# Call the function to start training\n",
        "train_model(generator, discriminator, images, descriptions, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2zNojjiptgf",
        "outputId": "3b69ec19-7710-4c7e-a826-f17895c08d46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [1/25], D Loss: 1.3553, G Loss: 0.7597\n",
            "Epoch [1/10], Step [11/25], D Loss: 0.3521, G Loss: 1.4991\n",
            "Epoch [1/10], Step [21/25], D Loss: 0.4122, G Loss: 1.3883\n",
            "End of Epoch [1/10], Avg D Loss: 0.5082, Avg G Loss: 1.3155\n",
            "Epoch [2/10], Step [1/25], D Loss: 0.1627, G Loss: 2.3886\n",
            "Epoch [2/10], Step [11/25], D Loss: 0.2948, G Loss: 1.7007\n",
            "Epoch [2/10], Step [21/25], D Loss: 0.5402, G Loss: 0.9791\n",
            "End of Epoch [2/10], Avg D Loss: 0.3507, Avg G Loss: 1.6763\n",
            "Epoch [3/10], Step [1/25], D Loss: 0.2997, G Loss: 1.6016\n",
            "Epoch [3/10], Step [11/25], D Loss: 0.1240, G Loss: 2.7200\n",
            "Epoch [3/10], Step [21/25], D Loss: 0.3504, G Loss: 2.6354\n",
            "End of Epoch [3/10], Avg D Loss: 0.2688, Avg G Loss: 2.5884\n",
            "Epoch [4/10], Step [1/25], D Loss: 0.0152, G Loss: 5.6730\n",
            "Epoch [4/10], Step [11/25], D Loss: 0.1697, G Loss: 3.6843\n",
            "Epoch [4/10], Step [21/25], D Loss: 0.1441, G Loss: 2.4302\n",
            "End of Epoch [4/10], Avg D Loss: 0.1603, Avg G Loss: 4.0194\n",
            "Epoch [5/10], Step [1/25], D Loss: 0.0720, G Loss: 3.5104\n",
            "Epoch [5/10], Step [11/25], D Loss: 0.1776, G Loss: 2.9965\n",
            "Epoch [5/10], Step [21/25], D Loss: 0.0396, G Loss: 3.5654\n",
            "End of Epoch [5/10], Avg D Loss: 0.0845, Avg G Loss: 3.2772\n",
            "Epoch [6/10], Step [1/25], D Loss: 0.1542, G Loss: 2.7160\n",
            "Epoch [6/10], Step [11/25], D Loss: 1.6537, G Loss: 2.7759\n",
            "Epoch [6/10], Step [21/25], D Loss: 0.1660, G Loss: 2.4551\n",
            "End of Epoch [6/10], Avg D Loss: 0.3412, Avg G Loss: 2.1934\n",
            "Epoch [7/10], Step [1/25], D Loss: 0.1251, G Loss: 2.4802\n",
            "Epoch [7/10], Step [11/25], D Loss: 0.3984, G Loss: 1.6545\n",
            "Epoch [7/10], Step [21/25], D Loss: 0.2080, G Loss: 3.1955\n",
            "End of Epoch [7/10], Avg D Loss: 0.5750, Avg G Loss: 2.0799\n",
            "Epoch [8/10], Step [1/25], D Loss: 0.0936, G Loss: 2.8006\n",
            "Epoch [8/10], Step [11/25], D Loss: 0.5193, G Loss: 3.0072\n",
            "Epoch [8/10], Step [21/25], D Loss: 2.7260, G Loss: 0.0745\n",
            "End of Epoch [8/10], Avg D Loss: 0.9194, Avg G Loss: 1.7603\n",
            "Epoch [9/10], Step [1/25], D Loss: 0.3707, G Loss: 2.6656\n",
            "Epoch [9/10], Step [11/25], D Loss: 1.0593, G Loss: 4.3324\n",
            "Epoch [9/10], Step [21/25], D Loss: 1.3741, G Loss: 0.7531\n",
            "End of Epoch [9/10], Avg D Loss: 0.7828, Avg G Loss: 3.1002\n",
            "Epoch [10/10], Step [1/25], D Loss: 3.6060, G Loss: 2.3098\n",
            "Epoch [10/10], Step [11/25], D Loss: 6.6776, G Loss: 6.3757\n",
            "Epoch [10/10], Step [21/25], D Loss: 0.1563, G Loss: 2.5324\n",
            "End of Epoch [10/10], Avg D Loss: 12.8157, Avg G Loss: 4.3679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample text encoder using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=100)  # Adjust max_features as needed\n",
        "text_samples = [\"Coffee Shop Logo\", \"Modern Logo\", \"Tech Company Logo\", \"Fashion Brand Logo\"]  # Add more samples as needed\n",
        "vectorizer.fit(text_samples)\n",
        "\n",
        "def encode_texts(texts):\n",
        "    return vectorizer.transform(texts).toarray()\n"
      ],
      "metadata": {
        "id": "f40jq708qr2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Define your vectorizer globally\n",
        "vectorizer = TfidfVectorizer(max_features=100)  # Ensure max_features is set to 100\n",
        "\n",
        "# Fit the vectorizer on training text samples\n",
        "text_samples = [\"Coffee Shop Logo\", \"Modern Logo\", \"Tech Logo\", \"Artistic Logo\"]  # Example training samples\n",
        "vectorizer.fit(text_samples)\n",
        "\n",
        "def encode_text(text):\n",
        "    text_vector = vectorizer.transform([text]).toarray()  # Transform text to array\n",
        "    return torch.FloatTensor(text_vector).to(device)  # Convert to tensor and move to device\n"
      ],
      "metadata": {
        "id": "L2YDVzifvmWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_logo(generator, text):\n",
        "    text_vector = encode_text(text)  # Encode the text\n",
        "    noise = torch.randn(1, 100).to(device)  # Generate noise of the correct size\n",
        "    input_vector = text_vector + noise  # Combine text vector with noise\n",
        "    generated_image = generator(input_vector)  # Generate image\n",
        "    return generated_image.view(-1, 3, 128, 128).cpu().detach().numpy()  # Reshape and move to CPU\n"
      ],
      "metadata": {
        "id": "Ox2dnmsGvotO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a logo for \"Coffee Shop Logo\"\n",
        "logo_image = generate_logo(generator, \"Coffee Shop Logo\")\n",
        "\n",
        "# Display the generated logo\n",
        "display_image(logo_image)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "R5r1DrFqvDj2",
        "outputId": "8cd73255-721c-45a9-951f-674df338a52d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_vector shape: torch.Size([1, 6]), noise shape: torch.Size([1, 100])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (6) must match the size of tensor b (100) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-4d054a1257c0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generate a logo for \"Coffee Shop Logo\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlogo_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_logo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Coffee Shop Logo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Display the generated logo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdisplay_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogo_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-87dc3e53caeb>\u001b[0m in \u001b[0;36mgenerate_logo\u001b[0;34m(generator, text)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"text_vector shape: {text_vector.shape}, noise shape: {noise.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0minput_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_vector\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise\u001b[0m  \u001b[0;31m# Combine text vector with noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mgenerated_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vector\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Generate image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgenerated_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reshape and move to CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (100) at non-singleton dimension 1"
          ]
        }
      ]
    }
  ]
}